{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Chapter9.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8DIUi8omNqZ"
      },
      "source": [
        "!wget https://github.com/endgameinc/malware_evasion_competition/blob/master/models/malconv/malconv.checkpoint?raw=true -O malconv.checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZQ3YXptnTPD"
      },
      "source": [
        "#　サンプルファイルのダウンロード\n",
        "!wget https://github.com/InQuest/malware-samples/blob/master/2019-02-Trickbot/374ef83de2b254c4970b830bb93a1dd79955945d24b824a0b35636e14355fe05?raw=true -O sample.bin\n",
        "!wget https://the.earth.li/~sgtatham/putty/latest/w32/putty.exe -O putty.exe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbsGCPfkYC9P"
      },
      "source": [
        "# liefの最新版だと、Colabがクラッシュすることがあるので 0.10.0を使用する\n",
        "!pip install lief==0.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt8vDQrhl2Ue"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# PyTorchでは、torch.nn.Moduleクラスを継承して独自のネットワークを作成する\n",
        "class MalConv(nn.Module):\n",
        "    # ニューラルネットワークの層を定義する\n",
        "    def __init__(self, out_size=2, channels=128, \\\n",
        "            window_size=512, embd_size=8):\n",
        "        # nn.Module内の初期化関数を実行する        \n",
        "        super(MalConv, self).__init__()\n",
        "\n",
        "        # 埋め込み層。各入力バイトを8次元ベクトルにマッピングする\n",
        "        self.embd = nn.Embedding(257, embd_size, padding_idx=0)\n",
        "        \n",
        "        # 一次元の畳み込み層2つ\n",
        "        self.window_size = window_size\n",
        "        self.conv_1 = nn.Conv1d(embd_size, channels, \n",
        "            window_size, stride=window_size, bias=True)\n",
        "        self.conv_2 = nn.Conv1d(embd_size, channels, \n",
        "            window_size, stride=window_size, bias=True)\n",
        "        \n",
        "        # プーリング層\n",
        "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
        "        \n",
        "        # 全結合層2つ\n",
        "        self.fc_1 = nn.Linear(channels, channels)\n",
        "        self.fc_2 = nn.Linear(channels, out_size)\n",
        "    \n",
        "    # 層間の計算を定義する\n",
        "    def forward(self, x):\n",
        "        # 入力を埋め込み層に与えた結果を得る\n",
        "        x = self.embd(x.long())\n",
        "        x = torch.transpose(x,-1,-2) # 行列の転置\n",
        "        \n",
        "        # 畳み込み層1の結果を得る\n",
        "        cnn_value = self.conv_1(x)\n",
        "\n",
        "        # 畳み込み層2の結果を取得し、シグモイド関数に掛ける\n",
        "        gating_weight = torch.sigmoid(self.conv_2(x))\n",
        "\n",
        "        # 積をとる\n",
        "        x = cnn_value * gating_weight\n",
        "        \n",
        "        # プーリング層の結果を得る\n",
        "        x = self.pooling(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # 平滑化\n",
        "        \n",
        "        # 全結合層1の結果をReLU関数に掛ける\n",
        "        x = F.relu(self.fc_1(x))\n",
        "\n",
        "        # 全結合層2の結果を得る\n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZOwiM7Ul5GK"
      },
      "source": [
        "MALCONV_MODEL_PATH = 'malconv.checkpoint'\n",
        "\n",
        "class MalConvModel(object):\n",
        "    def __init__(self, model_path, thresh=0.5, name='malconv'): \n",
        "        # MalConvのロード\n",
        "        self.model = MalConv(channels=256, \n",
        "            window_size=512, embd_size=8).train()\n",
        "        # 学習済のモデルをロードする\n",
        "        weights = torch.load(model_path,map_location='cpu')\n",
        "        self.model.load_state_dict(weights['model_state_dict'])\n",
        "        self.thresh = thresh\n",
        "        self.__name__ = name\n",
        "\n",
        "    def predict(self, bytez):\n",
        "        # ファイルのバイト列を整数（0～255）に変換する\n",
        "        _inp = torch.from_numpy( \\\n",
        "            np.frombuffer(bytez,dtype=np.uint8)[np.newaxis,:])\n",
        "        # MalConvの結果をソフトマックス関数に掛け、マルウェアらしさの確率を出力する\n",
        "        with torch.no_grad():\n",
        "            outputs = F.softmax(self.model(_inp), dim=-1)\n",
        "\n",
        "        # マルウェアらしさが閾値0.5を越えていたらマルウェアと判定する\n",
        "        return outputs.detach().numpy()[0,1] > self.thresh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLWZ4RxFnOqu"
      },
      "source": [
        "# ファイルのバイト列を取得する\n",
        "with open('sample.bin', 'rb') as f:\n",
        "    bytez = f.read()\n",
        "\n",
        "# MalConvModelを用いて分類する\n",
        "malconv = MalConvModel(MALCONV_MODEL_PATH, thresh=0.5)\n",
        "print(f'{malconv.__name__}:  {malconv.predict(bytez)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Rnxeqwnzch"
      },
      "source": [
        "!pip install pefile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8mun1N4np4o"
      },
      "source": [
        "from pefile import *\n",
        "\n",
        "# PEインスタンスを作成する\n",
        "pe = PE('sample.bin')\n",
        "\n",
        "# 生のバイト列にはメモリマップドファイルとしてアクセスできる\n",
        "# ここでは、DOSヘッダの冒頭に含まれる文字列「MZ」を取得する\n",
        "print(pe.__data__[0:2])\n",
        "\n",
        "# PEヘッダとそのメンバにはFILE_HEADERからアクセスできる\n",
        "print('NumberOfSection is {0}'.format(pe.FILE_HEADER.NumberOfSections))\n",
        "\n",
        "# オプショナルヘッダとそのメンバにはOPTIONAL_HEADERからアクセスできる\n",
        "print('AddressOfEntryPoint at 0x{0:08x}'.format(pe.OPTIONAL_HEADER.AddressOfEntryPoint))\n",
        "\n",
        "# データディレクトリはそれぞれDIRECTORY_ENTRY_IMPORT、\n",
        "# DIRECTORY_ENTRY_DEBUGなどからアクセスできる\n",
        "# ここでは、インポート関数のデータディレクトリから\n",
        "# インポートしているDLLとそのAPIを列挙する\n",
        "for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
        "    dll_name = entry.dll.decode('utf-8')\n",
        "    print(dll_name)\n",
        "    for func in entry.imports:\n",
        "        try:\n",
        "            print('\\t{0} at 0x{1:08x}'.format(func.name.decode('utf-8'), func.address))\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "# セクションテーブルは各セクションのインスタンスとしてアクセスできる\n",
        "# ここでは、各セクションの名前とPointerToRawDataを列挙する\n",
        "# また、各セクションのデータをget_dataメソッドを通じて取得する\n",
        "for section in pe.sections:\n",
        "    print(section.Name.decode('utf-8'))\n",
        "    print('\\tPointerToRawData at 0x{0:08x}'.format(section.PointerToRawData))\n",
        "    print('\\t{0} ...'.format(section.get_data()[0:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBIEPZ2kplBW"
      },
      "source": [
        "import datetime\n",
        "\n",
        "class PEManipulator(PE):\n",
        "    def __init__(self, name=None, data=None, fast_load=None):\n",
        "        super(PEManipulator, self).__init__(name, data, fast_load)\n",
        "\n",
        "    def reset_timestamp(self, new_timestamp_str=None):\n",
        "        # 指定された日時の文字列をUNIX時間に変換する\n",
        "        if new_timestamp_str == None:\n",
        "            new_timestamp_str = datetime.datetime.now().strftime( \\\n",
        "                '%Y-%m-%d %H:%M:%S')\n",
        "        new_timestamp = int(time.mktime(time.strptime( \\\n",
        "                new_timestamp_str, '%Y-%m-%d %H:%M:%S')))\n",
        "        # 当該UNIX時間でTimeDateStampを上書きする\n",
        "        self.FILE_HEADER.TimeDateStamp = new_timestamp \n",
        "        return\n",
        "\n",
        "pe = PEManipulator('sample.bin')\n",
        "pe.reset_timestamp(new_timestamp_str='2020-01-01 00:00:00')\n",
        "pe.write('sample_modified.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68ZNraLspN3g"
      },
      "source": [
        "import random\n",
        "\n",
        "class PEManipulator(PE):\n",
        "    def __init__(self, name=None, data=None, fast_load=None):\n",
        "        super(PEManipulator, self).__init__(name, data, fast_load)\n",
        "        # コードケイブのアドレスとサイズを記録する辞書を初期化する\n",
        "        self.code_cave_dict = {}\n",
        "    \n",
        "    def reset_timestamp(self, new_timestamp_str=None):\n",
        "        # 指定された日時の文字列をUNIX時間に変換する\n",
        "        if new_timestamp_str == None:\n",
        "            new_timestamp_str = datetime.datetime.now().strftime( \\\n",
        "                '%Y-%m-%d %H:%M:%S')\n",
        "        new_timestamp = int(time.mktime(time.strptime( \\\n",
        "                new_timestamp_str, '%Y-%m-%d %H:%M:%S')))\n",
        "        # 当該UNIX時間でTimeDateStampを上書きする\n",
        "        self.FILE_HEADER.TimeDateStamp = new_timestamp \n",
        "        return\n",
        "\n",
        "    def add_overlay(self, upper=255, overlay=None):\n",
        "        # 書き込むサイズを設定する\n",
        "        L = 2**random.randint(5, 8)\n",
        "        # 追加データを指定しない場合、\n",
        "        # 指定された文字コードとサイズの範囲でランダムなデータを生成する\n",
        "        if overlay == None:\n",
        "            overlay = bytes([random.randint(0, upper) for _ in range(L)])\n",
        "        # ファイルの末尾にデータを追記する\n",
        "        self.__data__ = (self.__data__[:-1] + overlay)\n",
        "        \n",
        "        return\n",
        "\n",
        "    def find_code_cave(self, min_cave_size=100):\n",
        "        # セクションごとにコードケイブを探索する\n",
        "        for section in self.sections:\n",
        "            code_cave_offset = 0\n",
        "            code_cave_size = 0\n",
        "\n",
        "            # セクションに含まれるデータを取得する\n",
        "            data = section.get_data()\n",
        "\n",
        "            for i, byte in enumerate(data):\n",
        "                code_cave_offset += 1\n",
        "\n",
        "                # 現在のデータが0x00ならコードケイブの候補とする\n",
        "                if byte == 0x00:\n",
        "                    code_cave_size += 1\n",
        "                    continue\n",
        "\n",
        "                # コードケイブの候補が一定のサイズ以上連続している場合、\n",
        "                elif code_cave_size > min_cave_size:\n",
        "                    # ヒューリスティック：コードケイブの候補間に\n",
        "                    # わずかなデータがあればコードケイブではないと判断し、\n",
        "                    # そうでなければコードケイブであると判断する\n",
        "                    if i < len(data)-1 and data[i+1] == 0x00:\n",
        "                        break\n",
        "                    \n",
        "                    # ファイル上のコードケイブの起点となるアドレスを取得し、\n",
        "                    code_cave_address = section.PointerToRawData \\\n",
        "                            + code_cave_offset - code_cave_size - 1\n",
        "                    # コードケイブの起点アドレスとコードケイブのサイズを登録する\n",
        "                    self.code_cave_dict.update({code_cave_address:\n",
        "                            code_cave_size})\n",
        "\n",
        "                code_cave_size = 0\n",
        "\n",
        "        return\n",
        "\n",
        "    def add_code_cave(self, upper=255, min_cave_size=100, code_cave=None):\n",
        "        # コードケイブ辞書が空の場合、コードケイブを探索する\n",
        "        if self.code_cave_dict == {}: self.find_code_cave(min_cave_size)\n",
        "        try:\n",
        "            # コードケイブ辞書からランダムに1件取得する\n",
        "            code_cave_address, code_cave_size = \\\n",
        "                random.choice(list(self.code_cave_dict.items()))\n",
        "            L = code_cave_size\n",
        "            # 追加データを指定しない場合、\n",
        "            # 指定された文字コードとサイズの範囲でランダムなデータを生成する\n",
        "            if code_cave == None:\n",
        "                code_cave = bytes([random.randint(0, upper) \\\n",
        "                    for _ in range(L)])\n",
        "            # 追加データを指定する場合、\n",
        "            # サイズの範囲に収まるように切り詰める\n",
        "            else:\n",
        "                code_cave = code_cave[0:L]\n",
        "            # 起点となるアドレスからコードケイブを追記する\n",
        "            self.set_bytes_at_offset(code_cave_address, code_cave)\n",
        "        except: # 辞書が空の場合は何もしない\n",
        "            pass\n",
        "\n",
        "        return\n",
        "\n",
        "pe = PEManipulator('sample.bin')\n",
        "pe.add_overlay()\n",
        "pe.add_code_cave()\n",
        "pe.write('sample_modified.exe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7woh280kp99c"
      },
      "source": [
        "!pip install keras-rl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXJxI5WFwdTU"
      },
      "source": [
        "!apt-get -qq -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dHZ9_ElwXM3"
      },
      "source": [
        "# Start virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbUh2KfPpB6u"
      },
      "source": [
        "# OpenAI Gymをインポートする\n",
        "import gym\n",
        "\n",
        "# KerasおよびKeras-RLをインポートする\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# 一部環境でのエラー抑制用イディオム\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# 強化学習タスクの環境を初期化する\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# Q関数の近似に用いるニューラルネットワークを定義する\n",
        "# 状態空間の次元env.observation_space.shapeを入力、\n",
        "# 行動空間の次元nb_actionsを出力としていれば、中間層は好きに積み重ねてよい\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "# Experience Replay用のメモリを用意する\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "\n",
        "# 行動ポリシーとして確率1-epsでランダムな行動をとらせるようにする\n",
        "policy = EpsGreedyQPolicy(eps=.1)\n",
        "\n",
        "# エージェントを初期化する\n",
        "# DQNAgentの引数にはさきほど定義したネットワーク、行動空間の次元数、\n",
        "# Exprerience Replay用のメモリ、割引率、Target Q-Networkのアップデート頻度、\n",
        "# 行動ポリシーなどを指定する\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, gamma=0.99, memory=memory, nb_steps_warmup=100,target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(learning_rate=1e-3), metrics=['mse'])\n",
        "\n",
        "# エージェントを学習させる\n",
        "# ここでは100,000回の行動を通じてQ関数を近似している\n",
        "history = dqn.fit(env=env, nb_steps=100000, visualize=True, verbose=2)\n",
        "\n",
        "# ニューラルネットワークの重みを保存する\n",
        "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
        "\n",
        "# 学習済のエージェントをテストする\n",
        "# ここでは5回のエピソードにわたってエージェントをテストしている\n",
        "dqn.test(env=env, nb_episodes=5, visualize=True)\n",
        "\n",
        "# 学習過程をプロットするためにmatplotlibをインポートする\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 学習過程の履歴を取得する\n",
        "nb_episode_steps = history.history['nb_episode_steps']\n",
        "episode_reward = history.history['episode_reward']\n",
        "\n",
        "# 取得した学習過程をプロットする\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(nb_episode_steps)\n",
        "plt.ylabel('step')\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(episode_reward)\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('reward')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVn4sD6RMSXi"
      },
      "source": [
        "!pip install git+https://github.com/elastic/ember.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKfIKqJbMcJ1"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import gym.spaces\n",
        "from ember import PEFeatureExtractor\n",
        "\n",
        "class MalwareEvasionEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 行動空間を定義する（必須）\n",
        "        # ここではreset_timestamp, add_overlay,\n",
        "        # add_code_cave, そしてadd_fake_importsの4つ \n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "        # 報酬の最小値と最大値を定義する（必須）\n",
        "        self.reward_range = [-1., 100.]\n",
        "        # 環境を初期化する（必須）\n",
        "        self.reset()\n",
        "\n",
        "    # 環境を初期化する（必須）\n",
        "    # 戻り値は初期状態\n",
        "    def reset(self):\n",
        "        # マルウェアのバイト列を読み取り、\n",
        "        with open('sample.bin', 'rb') as f:\n",
        "            malware_bytez = f.read()        \n",
        "        self.bytez = malware_bytez\n",
        "        # 4章で用いたEMBERの特徴抽出器を使って状態を生成する\n",
        "        # この特徴抽出の内部ではLIEFが用いられているが、\n",
        "        # pefileでも同様の処理は可能である\n",
        "        self.extractor = PEFeatureExtractor(2)\n",
        "        self.observation_space = np.array( \\\n",
        "            self.extractor.feature_vector(self.bytez), \\\n",
        "                dtype=np.float32)\n",
        "        # 分類器を初期化する\n",
        "        self.model = MalConvModel(MALCONV_MODEL_PATH, thresh=0.5)\n",
        "        return self.observation_space\n",
        "\n",
        "    # 行動を実行する（必須）\n",
        "    # 戻り値は次状態、報酬、エピソード終了フラグ、追加情報 \n",
        "    def step(self, action):\n",
        "        # エージェントが0, 1, 2, 3のどれかをactionとして渡してくるので、\n",
        "        # 愚直にPEManipulatorのメソッドと対応づけて実行する\n",
        "        pe = PEManipulator(data=self.bytez)\n",
        "        if action == 0:\n",
        "            pe.reset_timestamp()\n",
        "        if action == 1:\n",
        "            pe.add_overlay()\n",
        "        if action == 2:\n",
        "            pe.add_code_cave()\n",
        "        if action == 3:\n",
        "            pe.add_fake_imports()\n",
        "        \n",
        "        # 状態を更新する\n",
        "        self.bytez = pe.write()\n",
        "        self.observation_space = np.array( \\\n",
        "            self.extractor.feature_vector(self.bytez), \\\n",
        "                dtype=np.float32)\n",
        "        \n",
        "        # 報酬を取得する\n",
        "        # ここではMalConvがマルウェアだと判定すると-1、\n",
        "        # MalConvが良性ファイルだと判定すると100の報酬を与える\n",
        "        reward = -1 if self.model.predict(self.bytez) else 100\n",
        "        \n",
        "        # 報酬をもとにエピソード終了フラグを更新する\n",
        "        episode_over = False if reward == -1 else True\n",
        "        \n",
        "        return self.observation_space, reward, episode_over, {}\n",
        "\n",
        "    # 環境を可視化する（必須）\n",
        "    # 中身は空でよい\n",
        "    def render(self, mode='human', close=False):\n",
        "        pass\n",
        "\n",
        "    # 環境を閉じる\n",
        "    # 中身は空でよい\n",
        "    def _close(self):\n",
        "        pass\n",
        "\n",
        "    # シードを固定する\n",
        "    # 中身は空でよい\n",
        "    def _seed(self, seed=None):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8R0DrPBM68S"
      },
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "register(\n",
        "    # 環境のIDを登録する\n",
        "    # IDは<環境名>-v<バージョン番号>というフォーマットに沿って記述する\n",
        "    id='MalwareEvasionEnv-v0',\n",
        "    entry_point='__main__:MalwareEvasionEnv'\n",
        "    # エントリポイントを定義する\n",
        "    # エントリポイントは<名前空間>:<クラス名>というフォーマットに沿って記述する\n",
        "    # ここではJupyter NotebookまたはGoogle Colaboratoryのセル中に環境が存在することを前提としている\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntbUTUY7M_Ue"
      },
      "source": [
        "class MalConvModel(object):\n",
        "    def __init__(self, model_path, thresh=0.5, name='malconv'): \n",
        "    # MalConvのロード\n",
        "        self.model = MalConv(channels=256, \n",
        "        window_size=512, embd_size=8).train()\n",
        "    # 学習済のモデルをロードする\n",
        "        weights = torch.load(model_path,map_location='cpu')\n",
        "        self.model.load_state_dict(weights['model_state_dict'])\n",
        "        self.thresh = thresh\n",
        "        self.__name__ = name\n",
        "\n",
        "    def predict_with_score(self, bytez):\n",
        "        # ファイルのバイト列を整数（0～255）に変換する\n",
        "        _inp = torch.from_numpy( \\\n",
        "            np.frombuffer(bytez,dtype=np.uint8)[np.newaxis,:])\n",
        "        # MalConvの結果をソフトマックス関数に掛け、マルウェアらしさの確率を出力する\n",
        "        with torch.no_grad():\n",
        "            outputs = F.softmax(self.model(_inp), dim=-1)\n",
        "\n",
        "        # スコアを直接返す\n",
        "        return outputs.detach().numpy()[0,1]\n",
        "\n",
        "class MalwareEvasionEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 行動空間を定義する（必須）\n",
        "        # ここでは以下の13件の行動を用いる\n",
        "        # - reset_timestamp(現在日時)\n",
        "        # - add_overlay(ランダムに生成したデータ)\n",
        "        # - add_overlay(良性ファイルのデータ片)を5通り\n",
        "        # - add_code_cave(ランダムに生成したデータ)\n",
        "        # - add_code_cave(良性ファイルのデータ片)を5通り\n",
        "        self.action_space = gym.spaces.Discrete(13)\n",
        "        # 報酬の最小値と最大値を定義する（必須）\n",
        "        self.reward_range = [-1., 100.]\n",
        "        # 環境を初期化する（必須）\n",
        "        self.reset()\n",
        "\n",
        "    # 環境を初期化する（必須）\n",
        "    # 戻り値は初期状態\n",
        "    def reset(self):        \n",
        "        # マルウェアのバイト列を読み取り、\n",
        "        with open('sample.bin', 'rb') as f:\n",
        "            malware_bytez = f.read()\n",
        "        self.bytez = malware_bytez\n",
        "        # 4章で用いたEMBERの特徴抽出器を使って状態を生成する\n",
        "        # この特徴抽出の内部ではLIEFが用いられているが、\n",
        "        # pefileでも同様の処理は可能である\n",
        "        self.extractor = PEFeatureExtractor(2)\n",
        "        self.observation_space = np.array( \\\n",
        "            self.extractor.feature_vector(self.bytez), \\\n",
        "                dtype=np.float32)\n",
        "        \n",
        "        # 良性ファイルのバイト列を読み取り、\n",
        "        with open('putty.exe', 'rb') as f:\n",
        "            benign_bytez = f.read()\n",
        "        # 分割したデータのリストを作成する\n",
        "        num_splits = 5\n",
        "        offset = int(len(benign_bytez) / num_splits)\n",
        "        self.benign_bytez_list = \\\n",
        "            [benign_bytez[i: i+offset] \\\n",
        "                for i in range(0, len(benign_bytez), offset)]\n",
        "        \n",
        "        # 分類器を初期化する\n",
        "        self.model = MalConvModel(MALCONV_MODEL_PATH, thresh=0.5)\n",
        "        \n",
        "        # スコアを初期化する\n",
        "        self.prev_score = 1\n",
        "        self.thresh = 0.5\n",
        "        self.steps = 0\n",
        "        self.steps_limit = 100\n",
        "        \n",
        "        return self.observation_space\n",
        "\n",
        "  # 行動を実行する（必須）\n",
        "    # 戻り値は次状態、報酬、エピソード終了フラグ、追加情報 \n",
        "    def step(self, action):\n",
        "        # エージェントが0〜12のいずれかをactionとして渡してくるので、\n",
        "        # PEManipulatorのメソッドと対応づけて実行する\n",
        "        pe = PEManipulator(data=self.bytez)\n",
        "        if action == 0:\n",
        "            pe.reset_timestamp()\n",
        "        if action == 1:\n",
        "            pe.add_overlay()\n",
        "        # 良性ファイルの一部をオーバーレイとして追加する\n",
        "        if action in range(2, 6):\n",
        "            pe.add_overlay( \\\n",
        "                overlay=self.benign_bytez_list[action-2])\n",
        "        if action == 7:\n",
        "            pe.add_code_cave()\n",
        "        # 良性ファイルの一部をコードケイブとして追加する\n",
        "        if action in range(8, 12):\n",
        "            pe.add_code_cave( \\\n",
        "                code_cave=self.benign_bytez_list[action-8])\n",
        "        \n",
        "        # 状態を更新する\n",
        "        self.bytez = pe.write()\n",
        "        self.observation_space = np.array( \\\n",
        "            self.extractor.feature_vector(self.bytez), \\\n",
        "                dtype=np.float32)\n",
        "\n",
        "        # 報酬を取得する\n",
        "        # ここではMalConvの悪性スコアが\n",
        "        # 前回のスコアより低くなれば+1、低くならなければ-1、\n",
        "        # そしてMalConvが良性ファイルだと判定すると100の報酬を与える\n",
        "        # 報酬または行動回数に応じてエピソード終了フラグも更新する\n",
        "        episode_over = False\n",
        "        self.steps += 1\n",
        "\n",
        "        score = self.model.predict_with_score(self.bytez)\n",
        "        if score < self.prev_score:\n",
        "            reward = 1\n",
        "        if score < self.thresh:\n",
        "            reward = 100\n",
        "            episode_over = True\n",
        "        else:\n",
        "            reward = -1\n",
        "\n",
        "        self.prev_score = score\n",
        "\n",
        "        if self.steps > self.steps_limit:\n",
        "            episode_over = True\n",
        "        \n",
        "        # 報酬をもとにエピソード終了フラグを更新する\n",
        "        episode_over = False if reward == -1 else True\n",
        "\n",
        "        return self.observation_space, reward, episode_over, {}\n",
        "\n",
        "    # 環境を可視化する（必須）\n",
        "    # 中身は空でよい\n",
        "    def render(self, mode='human', close=False):\n",
        "        pass\n",
        "\n",
        "    # 環境を閉じる\n",
        "    # 中身は空でよい\n",
        "    def _close(self):\n",
        "        pass\n",
        "\n",
        "    # シードを固定する\n",
        "    # 中身は空でよい\n",
        "    def _seed(self, seed=None):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO_2Fs9tOP2U"
      },
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "register(\n",
        "    # 環境のIDを登録する\n",
        "    # IDは<環境名>-v<バージョン番号>というフォーマットに沿って記述する\n",
        "    id='MalwareEvasionEnv-v1',\n",
        "    entry_point='__main__:MalwareEvasionEnv'\n",
        "    # エントリポイントを定義する\n",
        "    # エントリポイントは<名前空間>:<クラス名>というフォーマットに沿って記述する\n",
        "    # ここではJupyter NotebookまたはGoogle Colaboratoryのセル中に環境が存在することを前提としている\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KybcrpzPNI8I"
      },
      "source": [
        "# OpenAI Gymをインポートする\n",
        "import gym\n",
        "\n",
        "# KerasおよびKeras-RLをインポートする\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# 一部環境でのエラー抑制用イディオム\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# 強化学習タスクの環境を初期化する\n",
        "ENV_NAME = 'MalwareEvasionEnv-v1'\n",
        "env = gym.make(ENV_NAME)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# Q関数の近似に用いるニューラルネットワークを定義する\n",
        "# 状態空間の次元env.observation_space.shapeを入力、\n",
        "# 行動空間の次元nb_actionsを出力としていれば、中間層は好きに積み重ねてよい\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "# Experience Replay用のメモリを用意する\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "\n",
        "# 行動ポリシーとして確率1-epsでランダムな行動をとらせるようにする\n",
        "policy = EpsGreedyQPolicy(eps=.1)\n",
        "\n",
        "# エージェントを初期化する\n",
        "# DQNAgentの引数にはさきほど定義したネットワーク、行動空間の次元数、\n",
        "# Exprerience Replay用のメモリ、割引率、Target Q-Networkのアップデート頻度、\n",
        "# 行動ポリシーなどを指定する\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
        "        gamma=0.99, target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(learning_rate=1e-3), metrics=['mse'])\n",
        "\n",
        "# エージェントを学習させる\n",
        "# ここでは100,000回の行動を通じてQ関数を近似している\n",
        "history = dqn.fit(env=env, nb_steps=100000, visualize=True, verbose=2)\n",
        "\n",
        "# ニューラルネットワークの重みを保存する\n",
        "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
        "\n",
        "# 学習済のエージェントをテストする\n",
        "# ここでは5回のエピソードにわたってエージェントをテストしている\n",
        "dqn.test(env=env, nb_episodes=5, visualize=True)\n",
        "\n",
        "# 学習過程をプロットするためにmatplotlibをインポートする\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 学習過程の履歴を取得する\n",
        "nb_episode_steps = history.history['nb_episode_steps']\n",
        "episode_reward = history.history['episode_reward']\n",
        "\n",
        "# 取得した学習過程をプロットする\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(nb_episode_steps)\n",
        "plt.ylabel('step')\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(episode_reward)\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('reward')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q2mB4NwdJa5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}